{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "data_path = \"/content/gdrive/MyDrive/austral/tesis/data/processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tifffile as tif\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CellDivisionDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.device = torch.device(device if device else \"cpu\")\n",
    "\n",
    "        # make binary labels, rtcc vs all the rest\n",
    "        self.img_labels_bin = self.img_labels.iloc[:, 1].apply(\n",
    "            lambda x: 1 if x in [3, 7, 8] else 0\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(str(self.img_dir), str(self.img_labels.iloc[idx, 0]))\n",
    "\n",
    "        # define the label\n",
    "        # label = self.img_labels.iloc[idx, 1]\n",
    "        label = self.img_labels_bin[idx]\n",
    "\n",
    "        # Use tifffile to read the stacked image directly\n",
    "        image_stack = tif.imread(img_path)\n",
    "\n",
    "        # If the image has a different dtype (e.g., uint16), you might want to convert it\n",
    "        # For example, converting to float and scaling to [0, 1] if necessary\n",
    "        # image_stack_tensor = image_stack_tensor.float() / image_stack_tensor.max()\n",
    "\n",
    "        image_stack = image_stack.astype(np.float32) / np.iinfo(image_stack.dtype).max\n",
    "\n",
    "        # If the image is not already in a torch tensor, convert it\n",
    "        # Assuming image_stack is a numpy array of shape [num_frames, H, W] for grayscale images\n",
    "        image_stack_tensor = torch.from_numpy(image_stack).to(self.device)\n",
    "\n",
    "        # Trim the tensor to have a fixed depth of 20 if it has more\n",
    "        if image_stack_tensor.shape[0] > 20:\n",
    "            image_stack_tensor = image_stack_tensor[:20, :, :]\n",
    "\n",
    "        # Add a channel dimension to the video tensor\n",
    "        image_stack_tensor = image_stack_tensor.unsqueeze(\n",
    "            0\n",
    "        )  # This adds the channel dimension at position 0\n",
    "\n",
    "        # Convert label to a tensor (if it's not already one)\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        # Apply any transforms here. Note: You'll need to modify or ensure your transforms\n",
    "        # can handle 3D data if they're meant for 2D images.\n",
    "        if self.transform:\n",
    "            # Apply transform to each frame individually or modify your transform to handle 3D data\n",
    "            # This is a placeholder; actual implementation will depend on your transform\n",
    "            pass\n",
    "\n",
    "        return image_stack_tensor, label\n",
    "\n",
    "    def show_stack(self, idx):\n",
    "        \"\"\"\n",
    "        Display a stack of images from a dataset.\n",
    "\n",
    "        This function takes the first sample from the provided dataset, assuming it is a tuple\n",
    "        where the first element is a stack of images (as a torch tensor) and the second element is its label.\n",
    "        It then plots the images in a 5x5 grid, displaying up to 25 images from the stack.\n",
    "\n",
    "        Parameters:\n",
    "        - dataset: A dataset object that allows indexing to retrieve a sample (image stack and label).\n",
    "\n",
    "        Returns:\n",
    "        None. This function directly displays the plotted images.\n",
    "        \"\"\"\n",
    "        img, label = self[idx]  # Extract the first sample (image stack and label)\n",
    "        # Set up the figure and axes for plotting\n",
    "        fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(15, 15))\n",
    "\n",
    "        # Flatten the axes array for easy iteration\n",
    "        axes_flat = axes.flatten()\n",
    "\n",
    "        for i, ax in enumerate(axes_flat):\n",
    "            if i < len(img):\n",
    "                # reshape removing the channel dimension\n",
    "                img = img.reshape(20, 75, 75)\n",
    "                ax.imshow(\n",
    "                    img.cpu().numpy()[i], cmap=\"gray\"\n",
    "                )  # Assuming img is a torch tensor\n",
    "                ax.set_title(f\"Frame {i+1}\")\n",
    "                ax.axis(\"off\")\n",
    "            else:\n",
    "                ax.axis(\"off\")  # Hide unused subplots\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def load_config(config_path: str = \"../config/base.yaml\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads a YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str, optional): The file path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The configuration settings as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from dataset import CellDivisionDataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(config, model):\n",
    "    # Load the dataset\n",
    "    dataset = CellDivisionDataset(\n",
    "        config[\"data\"][\"annotations_path\"],\n",
    "        config[\"data\"][\"img_path\"],\n",
    "        transform=None,\n",
    "        device=config[\"env\"][\"device\"],\n",
    "    )\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_size = int(config[\"train\"][\"size\"] * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # Create a generator on the required device\n",
    "    generator = torch.Generator(device=config[\"env\"][\"device\"])\n",
    "\n",
    "    # Use this generator in the random_split function\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # need to explicitly pass the generator to the dataloader for mps to work\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, generator=generator\n",
    "    )\n",
    "\n",
    "    # val_loader = DataLoader(\n",
    "    #     val_dataset, batch_size=32, shuffle=False, generator=generator\n",
    "    # )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = model.to(config[\"env\"][\"device\"])\n",
    "    model.train()\n",
    "\n",
    "    # Choose a loss function. For binary classification, BCELoss is commonly used.\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Choose an optimizer (e.g., Adam) and link it to your model's parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = config[\"train\"][\"epochs\"]\n",
    "    model = train_loop(num_epochs, model, criterion, optimizer, train_loader, config)\n",
    "\n",
    "\n",
    "def train_loop(num_epochs, model, criterion, optimizer, train_loader, config):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Make sure inputs and labels are on the same device as the model\n",
    "            inputs, labels = (\n",
    "                inputs.to(config[\"env\"][\"device\"]),\n",
    "                labels.to(config[\"env\"][\"device\"]),\n",
    "            )\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(\n",
    "                outputs.squeeze(), labels.float()\n",
    "            )  # Use .squeeze() to remove any extra dims\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # print every 10 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        # 3D Convolutional layers\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels=1, out_channels=16, kernel_size=(3, 3, 3), stride=1, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            in_channels=16, out_channels=32, kernel_size=(3, 3, 3), stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv3d(\n",
    "            in_channels=32, out_channels=64, kernel_size=(3, 3, 3), stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(in_features=64 * 9 * 9 * 5, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply 3D convolutions followed by pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # print(f\"Shape after convolutions: {x.shape}\")\n",
    "\n",
    "        # Dynamically calculate the correct number of features for fc1\n",
    "        # num_features = x.size(1) * x.size(2) * x.size(3) * x.size(4)\n",
    "        # print(f\"num_features: {num_features}\")\n",
    "        # now that I know the correct number, set it as in_features\n",
    "        num_features = 64 * 9 * 9 * 2\n",
    "\n",
    "        # Adjust the input size for the first fully connected layer based on the actual size\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=512).to(x.device)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(-1, num_features)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply sigmoid activation function to the output layer for binary classification\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Simple3DCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config/colab.yaml\")\n",
    "model = Simple3DCNN()\n",
    "model = train_model(config, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
